{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DICOManager Tutorial\n",
    "\n",
    "## Overview\n",
    "\n",
    "DICOManager is designed to sort, reconstruct, process and convert DICOM files to numpy arrays for use in Machine Learning and Deep Learning.\n",
    "\n",
    "## Sorting\n",
    "\n",
    "DICOManager begins with sorting DICOMs into a file tree with the following heirarchy:\n",
    "\n",
    "1. Cohort\n",
    "2. Patient\n",
    "3. Frame Of Reference\n",
    "4. Study\n",
    "5. Series\n",
    "6. Modality\n",
    "7. DICOM file\n",
    "\n",
    "File tree construction is automatic and can be called at any level using `groupings.<type>(files=<list_of_files>)`\n",
    "\n",
    "For example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohort: MyCohort\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import groupings\n",
    "\n",
    "files = glob(\"/list/to/dicoms/*.dcm\")\n",
    "cohort = groupings.Cohort(name=\"MyCohort\", files=files)\n",
    "print(cohort)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a grouping should only contain a subset of the dicoms in a directory, a filter_list can be used, where each parameter is specified as a list or dictionary.\n",
    "If no parameter is specified in the dictionary, dicoms will not be flitered by that field. \n",
    "\n",
    "For the parameter of struct name, either a list or dictionary can be provided. If a list is used, structures matching names within the list will be included. If a \n",
    "dict is provided, structures in each list will be mapped to the corresponding key.\n",
    "\n",
    "For example, below we create a filtering dictionary, titled filter_by, which specifies PatientID, Study date and Structure names which we want to filter the group by. Additionally, in this data set each StudyInstanceUID includes only a single image series, so we specify `include_series=False` to remove an unnecessary additional layer in the file tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_by={\"PatientID\": ['1', '2', 'N'],\n",
    "           \"StudyDate\": [\"19700101\", \"20211231\"],\n",
    "           \"SturctName\": {\"Struct1\": [\"Struct1\", \"Misspelled1\", \"AlternateName1\"],\n",
    "                          \"Struct2\": [\"Struct2\"]}}\n",
    "\n",
    "cohort = groupings.Cohort(name=\"MyCohort\", files=files, filter_by=filter_by, include_series=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For data sets of unknown completeness, filtering the group is of limited usefulness. Instead we want to only preserve groups which are \"complete\". We can check for completeness\n",
    "with the function `<group>.pull_incompletes`. This function takes a filter_by dictionary and returns a grouping of the excluded data. For example, if we wanted to specify that a complete node is a Frame Of Reference with at least one CT and one RTSTRUCT, we would use the following syntax. The result is that cohort has all incomplete leaves removed and returned as a separate tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_by = {\"Modality\": [\"CT\", \"RTSTRUCT\"]}\n",
    "\n",
    "incompletes = cohort.pull_incompletes(group=\"FrameOfRef\", contains=filter_by)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous example, if a Frame Of Reference had 2 CT and 1 RTSTRUCT, it would be considered complete because it contains at least one of each of the specified modalities. But,take, for example, a cohort where we want the number of modalities to be exactly 2 MR, 1 CT, 1 RTSTRUCT for a given patient. We can instead specify completeness as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_by = {\"Modality\": [\"CT\", \"RSTRUCT\", \"MR\", \"MR\"]}\n",
    "incompletes = cohort.pull_incompletes(group=\"Patient\", contains=filter_by, exact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, once we have the tree sorted as we desire, we can save the DICOM tree to a specified location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort.save_tree(path='/path/to/save/directory')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon each grouping level, the following basic functions can be used to alter the organization of the file tree, with `tree` refering to the current tree, `->` refering to the returned object:\n",
    "- `tree.merge(other) -> None`: Merge two trees\n",
    "- `tree.steal(other) -> None`: Moves a child of one tree to another tree\n",
    "- `tree.append(other) -> None`: Appends a node to another tree\n",
    "- `tree.prune(childname) -> None`: Removes a child from the tree\n",
    "- `tree.adopt(child) -> None`: Adopts a child to the current tree\n",
    "- `tree.flatten() -> None`: Restricts each parent to having one child within the tree\n",
    "- `tree.pop() -> <child type>`: Removes the first child from the tree and returns it\n",
    "- `save_dicoms(filepath) -> None`: Saves only the dicoms from the tree\n",
    "- `save_volumes(filepath) -> None`: Saves only the reconstructed volumes from the tree\n",
    "- `only_dicoms() -> bool`: Returns true if tree only contains dicoms\n",
    "- `has_dicoms() -> bool`: Returns true if tree contains dicoms\n",
    "- `has_volumes() -> bool`: Returns true if tree contains volumes\n",
    "- `iter_modalities() -> iterator`: Returns iterator of modalities within the tree\n",
    "- `iter_frames() -> iterator`: Returns iterator of frames of reference within the tree\n",
    "- `iter_dicoms() -> iterator`: Returns iterator of lists of dicoms for each series\n",
    "- `iter_volumes() -> iterator`: Returns iterator of each volume file within the tree\n",
    "- `iter_volume_date() -> iterator`: Returns iterator of each Modality.volumes_data dictionary within the tree\n",
    "- `iter_volumes_frames() -> iteratorr`: Returns an iterator of all volumes within each frame of references\n",
    "- `clear_dicoms() -> None`: Removes all dicoms from a tree\n",
    "- `clear_volumes() -> None`: Removes all volumes from a tree\n",
    "- `split_tree() -> tuple`: Splits the tree into a dicom only and volume only tree\n",
    "- `split_dicoms() -> tree`: Returns only a tree of dicoms, removes dicoms from source tree\n",
    "- `split_volumes() -> tree`: Returns only a tree of volumes, removes volumes from source tree\n",
    "- `volumes_to_pointers() -> None`: Converts all volumes to pointers, writing the arrays to disk\n",
    "- `pointers_to_volumes() -> None`: Converts all pointer to volumes, loading the arrays into memory\n",
    "- `recon(in_memory=False, parallelize=True) -> None`: Reconstructs all DICOMs within the tree, loading into memory or writing to disk at ~/.\n",
    "- `apply_tool(toolset, path=None, mod=None, warn=True) -> None`: A single threaded application of a list of tool functions, as specified in toolset\n",
    "- `apply_tools(toolset, path=None, nthreads=None) -> None`: Multithreaded application of a list of tool functions applied to the volumes within the tree\n",
    "- `pull_incompletes(group='Patient', exact=False, contains=None)`: Determines if a group is incomplete as defined by contains, returns the excluded groups from the tree. Original tree is modified in place.\n",
    "\n",
    "An example for a few is below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if the cohort contains dicoms\n",
    "print(cohort.has_dicoms())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten all patients\n",
    "for patient in cohort:\n",
    "    patient.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adopting the first 3 patients into a new cohort\n",
    "new_cohort = groupings.Cohort(name='NewCohort', files=None)\n",
    "\n",
    "for _ in range(3):\n",
    "    patient = cohort.pop()\n",
    "    new_cohort.adopt(patient)\n",
    "\n",
    "print(cohort)\n",
    "print(new_cohort)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstruction (DICOM to numpy)\n",
    "Reconstructing DICOMs into numpy arrays suitable for AI can be time consuming and tedious. DICOManager quickly converts DICOMs into volumes using parallelized processes. The simpliest way to reconstruct a patient or cohort is using the `.recon()` function. This function supports reconstruction of CT, MR, PET, NM, RTSTRUCT and RTDOSE files. Default behavior is to write the reconstructed volumes to disk and place a pointer within the tree indicating the volume location. If `.recon(in_memory=True)`, then the volume will be stored in the tree. This process is slower, does not allow for parallelization and can quickly consume an entire systems memory, but it is ideal for single patient inference or usage on systems where read-write access to disk is restricted. In some applications, parallelization across all CPU cores is not desired, in which case `.recon(parallelized=False)` can be used, at the expense of reconstruciton runtime. \n",
    "\n",
    "An example of reconstruction of our cohort is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cohort.recon(path='/path/to/save/volumes')\n",
    "print(new_cohort)\n",
    "\n",
    "new_cohort.pointers_to_volumes()\n",
    "print(new_cohort)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Manipulation\n",
    "Image manuplation of reconstructed volumes can be conducted on volumes which are stored within memory (for now). These image manuplations are then stored in the header fo the ReconstructedFile or ReconstructedVolume objects. The image manuplation functions, within `dicomanager.processing.tools` are:\n",
    "\n",
    "- `BiasFieldCorrection() -> Reconstructed{Volume, File}` <blockquote>Applies N4 Bias Field Correction to MR image volumes\n",
    "- `Crop(crop_size, centroid=None, centroids=None) -> ReconstructedVolume` <blockquote>Crops the volume to dimensions of crop_size around the center of the volume if no centroid(s) are specified. <br>\n",
    "&emsp;**centroid (list):** A single, common centroid <br>\n",
    "&emsp;**centroids (dict):** A dictionary with a centroid at each frame of reference UID can be given\n",
    "- `Normalize() -> Reconstructed{Volume, File}` <blockquote>Normalizes the reconstructedVolume\n",
    "- `Resample(ratio=None, dims=None, voxel_size=None, dz_limit=None, dz_goal=None, dz_exact=None) -> ReconstructedVolume` <blockquote> Resamples the image by either a specified ratio, dimensions, voxel size or desired slice thickness (dz). <br>\n",
    "&emsp;**ratio (list):** Resampling ratio, either per-dimension or uniform with r>1 upsampling and r<1 downsampling. Defaults to None. <br>\n",
    "&emsp;**dims (list):** Number of voxels per-dimension to resmaple to, with dimensions of None left unsampled. (e.g. [512, 512, None]). Defaults to None. <br>\n",
    "&emsp;**voxel_size (list):** Voxel size, in mm, to resample image. Defaults to None. <br>\n",
    "&emsp;**dz_limit (float):** Limited slice thickness, with dz > dz_limit being resampled to dz_goal or 1/2 slice thickness otherwise. Defaults to None. <br>\n",
    "&emsp;**dz_goal (float):** Resampling goal if dz_limit is specified. Defaults to None. <br>\n",
    "&emsp;**dz_exact (float):** Resamples all images to this exact dz, if specified. Will override dz_limit and dz_goal. Defaults to None. <br>\n",
    "- `Standardize() -> Reconstructed{Volume, File}` <blockquote> Standardizes the reconstructedVolume\n",
    "- `WindowLevel(window, level) -> Reconstructed{Volume, File}` <blockquote> Window and levels the reconstructed volume array. <br>\n",
    "&emsp;**window (float):** Window width <br>\n",
    "&emsp;**level (float):** Level value\n",
    "- `compute_centroids(tree, method, modalities=None, structures=None, struct_filter=None, volume_filter=None, nthreads=None, offset_fn=None) -> dict` <blockquote> Multithreaded computation of the centroid for each frame of reference. Returns dictionary keyed to FrameOrReferenceUID with centriod voxel location as value <br>\n",
    "&emsp;**tree (NodeMixin):** Tree to iterate through. <br>\n",
    "&emsp;**method (object):** Function to calculate centroid. Takes an array, returns a list of N items corresponding to the centroid in each axis. <br>\n",
    "&emsp;**modalities (list, optional):** Structure name to use for centroid. Defaults to None. <br>\n",
    "&emsp;**structures (list, optional):** List of str to calculate the centroid. Defaults to None. <br>\n",
    "&emsp;**struct_filter (object, optional):** A custom filtering function to pick structures, this is overriden if the paremeter structures is also specified. Function should accept a structure name and return a boolean for calculation of centroid. Defaults to None. <br>\n",
    "&emsp;**volume_filter (object, optional):** A custom filtering function to pick volumes, this is overriden if the parameter structures is also specified. Function should accept a ReconstructedVolume object and return a boolean for calculation of centroid. Defaults to None. <br>\n",
    "&emsp;**nthreads (int, optional):** Number of threads to use for the computation. Higher threads may run faster with higher memroy usage. Defaults to CPU count // 2. <br>\n",
    "&emsp;**offset_fn (object, optional):** A function to offset the centroid. Should accept a the centroid and a ReconstructedVolume object and return a centroid of equivalent dimensions. Defaults to None. <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our working cohort, say we intend to process the images prior to application in a deep learning pipeline. We can process them with the following order of tools:\n",
    "1. We interpolate the image for any missing slices during reconstruction\n",
    "2. Resample the images to 512x512 axial voxels while maintaining the same number of z-axis slices\n",
    "3. Window and level the image to window=500, level=250\n",
    "4. Normalize the image\n",
    "5. Crop the image around its center to 100x100x100 voxels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DICOManager import tools\n",
    "\n",
    "toolset = [tools.Interpolate(),\n",
    "           tools.Resample(dims=[512, 512, None]),\n",
    "           tools.WindowLevel(window=500, level=250),\n",
    "           tools.Normalize(),\n",
    "           tools.Crop(crop_size=[100, 100, 100])]\n",
    "\n",
    "cohort.apply_tools(toolset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also make these operations more powerful if needed.\n",
    "1. Interpolate the image, but allow for extrapolation if missing slices are along the exterior of the image\n",
    "2. Resample the image to 512x512 in the axial dimensions. If the slice thickness (dz) exceeds 2.39mm, we resample it by 0.5x (doubling number of axial slices)\n",
    "3. Normalize the image\n",
    "Then we can apply these opperations, same as before.\n",
    "\n",
    "But, say we want to now compute a cropping centroid not at the center of the volume but instead defined by a structure. We should compute these values following\n",
    "any resampling and interpolation to ensure our voxel location cropping centroid is not shifted by resampling.\n",
    "1. We can calculate the centroid for each frame of reference based on the specified structure name (or list of names, using whichever is found first), with the\n",
    "centroid computed using a specified method. If a custom method is specified, the method should take a numpy array as input and return a list of voxel locations.\n",
    "2. We can then apply these computed centroids during the cropping operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import center_of_mass\n",
    "\n",
    "# Compute the centroids for cropping\n",
    "\n",
    "toolset = [tools.Interpolate(extrapolate=True),\n",
    "           tools.Resample(dims=[512, 512, None], dz_limit=2.39),\n",
    "           tools.Normalize()]\n",
    "\n",
    "cohort.apply_tools(toolset)\n",
    "\n",
    "# Then compute the cropping size on the newly interpolated / resampled images\n",
    "centroids = tools.calculate_centroid(tree=cohort, modalities='RTSTRUCT', structure='body', method=center_of_mass)\n",
    "cropping = [tools.Crop(crop_size=[100, 100, 100], centroids=centroids)]\n",
    "\n",
    "cohort.apply_tools(cropping)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deconstruction (Numpy to RTSTRUCT)\n",
    "In some instances, particularly inference, conversion from boolean numpy array to an RTSTRUCT is desired. For this to be possible, the associated image DICOM files must be contained within the tree. Deconstruction is only possible at the Frame Of Reference level, or lower, as the RTSTRUCT must contain the equivalent dimensions to a CT group. If a Frame of Reference contains more than one CT, deconstruction is not currently supported. \n",
    "\n",
    "Deconstruction can occur as:\n",
    "- `from_ct()`: Creates a new RTSTRUCT from a CT files based upon the CT header\n",
    "- `from_rt()`: Creates a new RTSTRUCT from an existin RTSTRUCT file\n",
    "- `to_rt()`: Appends a segmentation to an existing RTSTRUCT file\n",
    "\n",
    "An example of deconstruction is provided below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "frame = new_cohort.iter_frames()[0]  # One frame of reference from the cohort\n",
    "vol = frame.iter_volumes()[0]  # One volume from the frame of reference\n",
    "rtstruct = np.zeros((1, vol.shape))  # Create an example array for demonstration\n",
    "\n",
    "print(f'before: {frame}')\n",
    "frame.decon.from_ct(rtstruct)\n",
    "print(f'after: {frame}')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c76933cfbe346ee382be0aefab6c87c10035cb793a59c5b7fb7184d7b059d578"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('tf23': conda)",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}