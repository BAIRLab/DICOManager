{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DICOManager Tutorial\n",
    "\n",
    "## Overview\n",
    "\n",
    "DICOManager is designed to sort, reconstruct, process and convert DICOM files to numpy arrays for use in Machine Learning and Deep Learning.\n",
    "\n",
    "## Sorting\n",
    "\n",
    "DICOManager begins with sorting DICOMs into a file tree with the following heirarchy:\n",
    "\n",
    "1. Cohort\n",
    "2. Patient\n",
    "3. Frame Of Reference\n",
    "4. Study\n",
    "5. Series\n",
    "6. Modality\n",
    "7. DICOM file\n",
    "\n",
    "File tree construction is automatic and can be called at any level using `groupings.<type>(files=<list_of_files>)`\n",
    "\n",
    "For example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohort: MyCohort\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import groupings\n",
    "\n",
    "files = glob(\"/list/to/dicoms/*.dcm\")\n",
    "cohort = groupings.Cohort(name=\"MyCohort\", files=files)\n",
    "print(cohort)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a grouping should only contain a subset of the dicoms in a directory, a filter_list can be used, where each parameter is specified as a list or dictionary.\n",
    "If no parameter is specified in the dictionary, dicoms will not be flitered by that field. \n",
    "\n",
    "For the parameter of struct name, either a list or dictionary can be provided. If a list is used, structures matching names within the list will be included. If a \n",
    "dict is provided, structures in each list will be mapped to the corresponding key.\n",
    "\n",
    "For example, below we create a filtering dictionary, titled filter_by, which specifies PatientID, Study date and Structure names which we want to filter the group by. Additionally, in this data set each StudyInstanceUID includes only a single image series, so we specify `include_series=False` to remove an unnecessary additional layer in the file tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_by={\"PatientID\": ['1', '2', 'N'],\n",
    "           \"StudyDate\": [\"19700101\", \"20211231\"],\n",
    "           \"SturctName\": {\"Struct1\": [\"Struct1\", \"Misspelled1\", \"AlternateName1\"],\n",
    "                          \"Struct2\": [\"Struct2\"]}}\n",
    "\n",
    "cohort = groupings.Cohort(name=\"MyCohort\", files=files, filter_by=filter_by, include_series=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For data sets of unknown completeness, filtering the group is of limited usefulness. Instead we want to only preserve groups which are \"complete\". We can check for completeness\n",
    "with the function `<group>.pull_incompletes`. This function takes a filter_by dictionary and returns a grouping of the excluded data. For example, if we wanted to specify that a complete node is a Frame Of Reference with at least one CT and one RTSTRUCT, we would use the following syntax. The result is that cohort has all incomplete leaves removed and returned as a separate tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_by = {\"Modality\": [\"CT\", \"RTSTRUCT\"]}\n",
    "\n",
    "incompletes = cohort.pull_incompletes(group=\"FrameOfRef\", contains=filter_by)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous example, if a Frame Of Reference had 2 CT and 1 RTSTRUCT, it would be considered complete because it contains at least one of each of the specified modalities. But,take, for example, a cohort where we want the number of modalities to be exactly 2 MR, 1 CT, 1 RTSTRUCT for a given patient. We can instead specify completeness as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_by = {\"Modality\": [\"CT\", \"RSTRUCT\", \"MR\", \"MR\"]}\n",
    "incompletes = cohort.pull_incompletes(group=\"Patient\", contains=filter_by, exact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, once we have the tree sorted as we desire, we can save the DICOM tree to a specified location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort.save_tree(path='/path/to/save/directory')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon each grouping level, the following basic functions can be used to alter the organization of the file tree, with `tree` refering to the current tree, `->` refering to the returned object:\n",
    "- `tree.merge(other) -> None`: Merge two trees \n",
    "- `tree.steal(other) -> None`: Moves a child of one tree to another tree\n",
    "- `tree.append(other) -> None`: Appends a node to another tree \n",
    "- `tree.prune(childname) -> None`: Removes a child from the tree \n",
    "- `tree.adopt(child) -> None`: Adopts a child to the current tree\n",
    "- `tree.flatten() -> None`: Restricts each parent to having one child within the tree \n",
    "- `tree.pop() -> <child type>`: Removes the first child from the tree and returns it \n",
    "- `save_dicoms(filepath) -> None`: Saves only the dicoms from the tree \n",
    "- `save_volumes(filepath) -> None`: Saves only the reconstructed volumes from the tree \n",
    "- `only_dicoms() -> bool`: Returns true if tree only contains dicoms\n",
    "- `has_dicoms() -> bool`: Returns true if tree contains dicoms\n",
    "- `has_volumes() -> bool`: Returns true if tree contains volumes\n",
    "- `iter_modalities() -> iterator`: Returns iterator of modalities within the tree\n",
    "- `iter_frames() -> iterator`: Returns iterator of frames of reference within the tree\n",
    "- `iter_dicoms() -> iterator`: Returns iterator of lists of dicoms for each series\n",
    "- `iter_volumes() -> iterator`: Returns iterator of each volume within the tree\n",
    "- `iter_volumes_frames() -> iteratorr`: Returns an iterator of all volumes within each frame of references\n",
    "- `clear_dicoms() -> None`: Removes all dicoms from a tree\n",
    "- `clear_volumes() -> None`: Removes all volumes from a tree\n",
    "- `split_tree() -> tuple`: Splits the tree into a dicom only and volume only tree\n",
    "- `split_dicoms() -> tree`: Returns only a tree of dicoms, removes dicoms from source tree\n",
    "- `split_volumes() -> tree`: Returns only a tree of volumes, removes volumes from source tree\n",
    "- `volumes_to_pointers() -> None`: Converts all volumes to pointers, writing the arrays to disk\n",
    "- `pointers_to_volumes() -> None`: Converts all pointer to volumes, loading the arrays into memory \n",
    "- `recon(in_memory=False, parallelize=True) -> None`: Reconstructs all DICOMs within the tree, loading into memory or writing to disk at ~/.\n",
    "\n",
    "An example for a few is below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if the cohort contains dicoms\n",
    "print(cohort.has_dicoms())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten all patients\n",
    "for patient in cohort:\n",
    "    patient.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adopting the first 3 patients into a new cohort\n",
    "new_cohort = groupings.Cohort(name='NewCohort', files=None)\n",
    "\n",
    "for _ in range(3):\n",
    "    patient = cohort.pop()\n",
    "    new_cohort.adopt(patient)\n",
    "\n",
    "print(cohort)\n",
    "print(new_cohort)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstruction\n",
    "Reconstructing DICOMs into numpy arrays suitable for AI can be time consuming and tedious. DICOManager quickly converts DICOMs into volumes using parallelized processes. The simpliest way to reconstruct a patient or cohort is using the `.recon()` function. This function supports reconstruction of CT, MR, PET, NM, RTSTRUCT and RTDOSE files. Default behavior is to write the reconstructed volumes to disk and place a pointer within the tree indicating the volume location. If `.recon(in_memory=True)`, then the volume will be stored in the tree. This process is slower, does not allow for parallelization and can quickly consume an entire systems memory, but it is ideal for single patient inference or usage on systems where read-write access to disk is restricted. In some applications, parallelization across all CPU cores is not desired, in which case `.recon(parallelized=False)` can be used, at the expense of reconstruciton runtime. \n",
    "\n",
    "An example of reconstruction of our cohort is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cohort.recon(path='/path/to/save/volumes')\n",
    "print(new_cohort)\n",
    "\n",
    "new_cohort.pointers_to_volumes()\n",
    "print(new_cohort)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Manipulation\n",
    "Image manuplation of reconstructed volumes can be conducted on volumes which are stored within memory (for now). These image manuplations are then stored in the header fo the ReconstructedFile or ReconstructedVolume objects. The image manuplation functions, within `dicomanager.processing.tools` are:\n",
    "\n",
    "- `dose_max_points(dose_array, dose_coords)` -> np.ndarray: Returns the maximum point in index, or coordinates, of the dose array\n",
    "- `window_level(window, level)` -> ReconstructedVolume: Window and levels the reconstructed volume array\n",
    "- `normalize(img)` -> RecontructedVolume: Normalizes the reconstructedVolume\n",
    "- `standardize(img) -> ReconstructedVolume`: Standardizes the reconstructedVolume\n",
    "- `crop(img, centroid, crop_size) -> ReconstructedVolume`: Crops the ReconstructedVolume around the centroid to dimensions of crop_size. Will offset from centroid to maintain crop_size dimensions. Will add option for padding with zeros instead in a later revision of the function.\n",
    "- `resample(img, ratio) -> ReconstructedVolume`: Resamples the image by the ratio provided\n",
    "- `bias_field_correction(img) -> ReconstructedVolume`: Uses N4BiasFieldCorrection from SimpleITK\n",
    "\n",
    "Exmaple of cropping and window, leveling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our working cohort, say we intend to process the images prior to application in a deep learning pipeline. We can process them with the following order of tools:\n",
    "1. We interpolate the image for any missing slices during reconstruction\n",
    "2. Resample the images to 512x512 axial voxels while maintaining the same number of z-axis slices\n",
    "3. Window and level the image to window=500, level=250\n",
    "4. Normalize the image\n",
    "5. Crop the image around its center to 100x100x100 voxels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DICOManager import tools\n",
    "\n",
    "toolset = [tools.Interpolate(),\n",
    "           tools.Resample(dims=[512, 512, None]),\n",
    "           tools.WindowLevel(window=500, level=250),\n",
    "           tools.Normalize(),\n",
    "           tools.Crop(crop_size=[100, 100, 100])]\n",
    "\n",
    "cohort.apply_tools(toolset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also make these operations more powerful if needed.\n",
    "1. Interpolate the image, but allow for extrapolation if missing slices are along the exterior of the image\n",
    "2. Resample the image to 512x512 in the axial dimensions. If the slice thickness (dz) exceeds 2.39mm, we resample it by 0.5x (doubling number of axial slices)\n",
    "3. Normalize the image\n",
    "Then we can apply these opperations, same as before.\n",
    "\n",
    "But, say we want to now compute a cropping centroid not at the center of the volume but instead defined by a structure. We should compute these values following\n",
    "any resampling and interpolation to ensure our voxel location cropping centroid is not shifted by resampling.\n",
    "1. We can calculate the centroid for each frame of reference based on the specified structure name (or list of names, using whichever is found first), with the\n",
    "centroid computed using a specified method. If a custom method is specified, the method should take a numpy array as input and return a list of voxel locations.\n",
    "2. We can then apply these computed centroids during the cropping operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import center_of_mass\n",
    "\n",
    "# Compute the centroids for cropping\n",
    "\n",
    "toolset = [tools.Interpolate(extrapolate=True),\n",
    "           tools.Resample(dims=[512, 512, None], dz_limit=2.39),\n",
    "           tools.Normalize()]\n",
    "\n",
    "cohort.apply_tools(toolset)\n",
    "\n",
    "# Then compute the cropping size on the newly interpolated / resampled images\n",
    "centroids = tools.calculate_centroid(tree=cohort, modalities='RTSTRUCT', structure='body' method=center_of_mass)\n",
    "cropping = [tools.Crop(crop_size=[100, 100, 100], centroids=centroids)]\n",
    "\n",
    "cohort.apply_tools(cropping)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deconstruction (Numpy to RTSTRUCT)\n",
    "In some instances, particularly inference, conversion from boolean numpy array to an RTSTRUCT is desired. For this to be possible, the associated image DICOM files must be contained within the tree. Deconstruction is only possible at the Frame Of Reference level, or lower, as the RTSTRUCT must contain the equivalent dimensions to a CT group. If a Frame of Reference contains more than one CT, deconstruction is not currently supported. \n",
    "\n",
    "Deconstruction can occur as:\n",
    "- `from_ct()`: Creates a new RTSTRUCT from a CT files based upon the CT header\n",
    "- `from_rt()`: Creates a new RTSTRUCT from an existin RTSTRUCT file\n",
    "- `to_rt()`: Appends a segmentation to an existing RTSTRUCT file\n",
    "\n",
    "An example of deconstruction is provided below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = new_cohort.iter_frames()[0]  # One frame of reference from the cohort\n",
    "vol = frame.iter_volumes()[0]  # One volume from the frame of reference\n",
    "rtstruct = np.zeros((1, vol.shape))  # Create an example array for demonstration\n",
    "\n",
    "print(f'before: {frame}')\n",
    "frame.decon.from_ct(rtstruct)\n",
    "print(f'after: {frame}')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c76933cfbe346ee382be0aefab6c87c10035cb793a59c5b7fb7184d7b059d578"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('tf23': conda)",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}